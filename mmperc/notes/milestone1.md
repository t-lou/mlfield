# Milestone: Multiâ€‘Modal Singleâ€‘Task BEV Detector

This milestone introduces a **multiâ€‘modal BEV detection network** built collaboratively with Copilot.  
The model fuses **LiDAR** and **camera** information to predict **3D bounding boxes** in the A2D2 dataset.

---

## ðŸ“¥ Inputs

- **LiDAR point cloud** (single sensor)
- **RGB camera image** (single sensor)

---

## ðŸ“¤ Output

- **3D bounding box predictions** in A2D2 format  
  (center, size, yaw)

---

## ðŸ§± Model Architecture

The system follows a simple but extensible design:

- **LiDAR encoder** â†’ BEV feature map  
- **Camera encoder** â†’ token embeddings  
- **Crossâ€‘modal fusion** (FuTrâ€‘style attention)  
- **Detection heads** â†’ heatmap + regression

![Start](./imgs/milestone1_futr.png)

---

## ðŸ”¥ Heatmap Evolution

Training was performed on the **A2D2â€‘preview** subset (10 frames).  
Below is a comparison between an early epoch and epoch 50, showing how the predicted heatmap becomes sharper and more aligned with the ground truth.

![Start](./imgs/milestone1_featuremap.png)

---

## ðŸš€ Next Steps

### Experiments
- Compare **multiâ€‘modal** vs **LiDARâ€‘only** vs **cameraâ€‘only**
- Add a second task (e.g., depth, semantic map, velocity)

### Engineering Improvements
- Switch to the **full A2D2 dataset** (with optional frame filtering)
- Improve and unify the data loader
- Make the model **fully configurable** (modality, backbone, stride, etc.)

---

If you want, I can help you turn this into a more formal project README with installation instructions, training commands, diagrams, and a roadmap.
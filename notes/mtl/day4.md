Perfect â€” Day 4 is where everything becomes **real**.  
Today youâ€™ll **build a working multiâ€‘task learning system**: a shared backbone, two task heads, dynamic loss balancing, and the logging you need to diagnose gradient conflict.

Iâ€™ll give you a clean, practical blueprint that fits your engineering style: minimal, reproducible, and easy to extend.

---

# ğŸŸ§ **DAY 4 â€” Build a Real Multiâ€‘Task Model**  
*Goal: implement a full MTL pipeline with shared backbone, two heads, and dynamic loss balancing.*

---

# 1ï¸âƒ£ Choose a Dataset (fast + practical)

You want something that:
- has multiple labels per image  
- is small enough to train quickly  
- demonstrates real MTL dynamics  

### **Recommended options**
### **Option A â€” NYUv2 (best for MTL)**
- RGB â†’ segmentation + depth + normals  
- Classic MTL benchmark  
- Shows gradient conflict clearly  

### **Option B â€” Cityscapes**
- segmentation + instance segmentation  
- closer to autonomous driving  

### **Option C â€” CIFARâ€‘10 (toy MTL)**
- classification + coarse label (superclass)  
- fastest to iterate  

If you want speed, start with **CIFARâ€‘10**.  
If you want realism, start with **NYUv2**.

---

# 2ï¸âƒ£ Architecture Blueprint (shared backbone + two heads)

Hereâ€™s the exact structure you should implement today.

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Input RGB    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Shared Backboneâ”‚  â† ResNet50 / Swin / ViT
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Seg Head      â”‚               â”‚ Depth Head     â”‚
â”‚ (decoder)     â”‚               â”‚ (decoder)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼                               â–¼
 Segmentation Map                 Depth Map
```

### **Backbone**
- ResNet50 or Swinâ€‘T  
- Pretrained weights recommended  
- Freeze first 1â€“2 stages for stability  

### **Heads**
- Segmentation head:  
  - FPN â†’ 1Ã—1 conv â†’ upsample â†’ softmax  
- Depth head:  
  - FPN â†’ 1Ã—1 conv â†’ upsample â†’ regression  

### **Why this works**
- Shared features capture geometry + semantics  
- Heads specialize  
- FPN gives multiâ€‘scale features for both tasks  

---

# 3ï¸âƒ£ Implement Dynamic Loss Balancing

Start with **uncertainty weighting** (Day 3â€™s default).

### **Loss**
\[
L = \frac{1}{2\sigma_s^2} L_{seg} + \frac{1}{2\sigma_d^2} L_{depth} + \log \sigma_s + \log \sigma_d
\]

Where:
- \( \sigma_s \) and \( \sigma_d \) are learnable scalars  
- They automatically adjust task weights  

### **Why this is perfect for Day 4**
- No hyperparameter tuning  
- Very stable  
- Works for heterogeneous tasks  

---

# 4ï¸âƒ£ Add Gradient Logging (your debugging superpower)

Log:
- gradient norms per task  
- cosine similarity between task gradients  
- perâ€‘task loss curves  
- backbone gradient variance  

This gives you:
- early detection of negative transfer  
- insight into whether PCGrad or GradNorm is needed  
- understanding of which layers conflict  

Youâ€™ll use this heavily in Day 5.

---

# 5ï¸âƒ£ Training Loop (the minimal version)

### **Steps**
1. Forward pass through backbone  
2. Forward pass through each head  
3. Compute each task loss  
4. Combine with uncertainty weighting  
5. Backprop once  
6. Log gradients  
7. Update backbone + heads + Ïƒ parameters  

### **Training tips**
- Use separate optimizers for heads if unstable  
- Use gradient clipping (1.0)  
- Use mixed precision for speed  
- Freeze early backbone layers for first 1â€“2 epochs  

---

# 6ï¸âƒ£ Evaluation (simple but essential)

Evaluate each task separately:
- segmentation: mIoU  
- depth: RMSE or scaleâ€‘invariant loss  

Track:
- whether both tasks improve  
- whether one task collapses  
- whether dynamic weights stabilize  

If one task collapses â†’ gradient conflict â†’ use PCGrad tomorrow.

---

# 7ï¸âƒ£ Your Dayâ€‘4 Deliverable

By the end of today, you should have:

### **A working MTL training script** with:
- shared backbone  
- two heads  
- uncertainty weighting  
- gradient logging  

### **A short note** on:
- which task dominates  
- whether gradients conflict  
- whether uncertainty weighting stabilizes training  

This sets you up perfectly for Day 5 (advanced topics + scaling).

---

# If you want, I can now give you:
- a **clean PyTorch code template** for this exact architecture  
- a **DETRâ€‘style multiâ€‘decoder version**  
- a **Cityscapesâ€‘specific MTL design**  
- a **gradient logging snippet**  
- or move straight to **Day 5**  

Just tell me what direction you want.
